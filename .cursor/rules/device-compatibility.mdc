# è®¡ç®—è®¾å¤‡å…¼å®¹æ€§è¦æ±‚

## å¼€å‘ä¸éƒ¨ç½²ç¯å¢ƒ

### å¼€å‘ç¯å¢ƒ
- **ä¸»è¦å¼€å‘è®¾å¤‡**: M4 èŠ¯ç‰‡ Mac Air 2025
- **æœ¬åœ°è®¡ç®—åç«¯**: Apple Silicon MPS (Metal Performance Shaders)
- **ç‰¹ç‚¹**: æ— CUDAæ”¯æŒï¼Œä½†æœ‰é«˜æ•ˆçš„MPSåŠ é€Ÿ

### ç›®æ ‡éƒ¨ç½²ç¯å¢ƒ  
- **ä¸»è¦éƒ¨ç½²å¹³å°**: Linuxç³»ç»Ÿ
- **è®¡ç®—åç«¯**: CUDA GPUåŠ é€Ÿ
- **ç‰¹ç‚¹**: æ”¯æŒNVIDIA GPUå’ŒCUDAç”Ÿæ€ç³»ç»Ÿ

## è·¨å¹³å°å…¼å®¹æ€§åŸåˆ™

### ğŸ”§ è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ä¸é€‰æ‹©
æ‰€æœ‰ä»£ç å¿…é¡»å®ç°æ™ºèƒ½çš„è®¾å¤‡æ£€æµ‹é€»è¾‘ï¼š

```python
def _get_optimal_device() -> torch.device:
    """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")  # Apple Silicon
    else:
        return torch.device("cpu")
```

### ğŸ“± è®¾å¤‡ç‰¹å®šä¼˜åŒ–

#### MPS (Apple Silicon) ä¼˜åŒ–
- **å†…å­˜ç®¡ç†**: MPSå†…å­˜æ¨¡å‹ä¸CUDAä¸åŒï¼Œéœ€ç‰¹æ®Šå¤„ç†
- **æ•°æ®ç±»å‹**: æŸäº›æ“ä½œåœ¨MPSä¸Šå¯èƒ½ä¸æ”¯æŒfloat16
- **åŒæ­¥æœºåˆ¶**: ä½¿ç”¨`torch.mps.synchronize()`è€Œé`torch.cuda.synchronize()`

#### CUDA (Linux) ä¼˜åŒ–
- **å¤šGPUæ”¯æŒ**: åˆ©ç”¨`torch.cuda.device_count()`è¿›è¡Œå¤šå¡å¹¶è¡Œ
- **å†…å­˜æ± **: ä½¿ç”¨CUDAå†…å­˜æ± ä¼˜åŒ–å¤§æ‰¹é‡è®¡ç®—
- **æ··åˆç²¾åº¦**: å……åˆ†åˆ©ç”¨Tensor CoreåŠ é€Ÿ

#### CPU åå¤‡æ”¯æŒ
- **BLASä¼˜åŒ–**: ç¡®ä¿OpenMPå’ŒMKLæ­£ç¡®é…ç½®
- **æ‰¹é‡å¤§å°**: CPUç¯å¢ƒä¸‹è‡ªåŠ¨è°ƒæ•´ä¸ºè¾ƒå°æ‰¹é‡

## ä»£ç å®ç°è¦æ±‚

### âœ… å¼ºåˆ¶è¦æ±‚

#### è®¾å¤‡æ— å…³çš„å¼ é‡æ“ä½œ
```python
# âœ… æ­£ç¡®ï¼šè®¾å¤‡æ— å…³çš„å®ç°
def compute_vector_field(self, x_0, x_1, t):
    device = x_0.device
    # æ‰€æœ‰è®¡ç®—éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šè¿›è¡Œ
    result = torch.einsum('bi,bj->bij', x_0, x_1)
    return result.to(device)

# âŒ é”™è¯¯ï¼šç¡¬ç¼–ç è®¾å¤‡
def compute_vector_field(self, x_0, x_1, t):
    result = torch.einsum('bi,bj->bij', x_0.cuda(), x_1.cuda())
    return result
```

#### è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥
```python
def validate_device_consistency(self, *tensors):
    """ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š"""
    devices = {t.device for t in tensors if torch.is_tensor(t)}
    if len(devices) > 1:
        raise RuntimeError(f"å¼ é‡åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Š: {devices}")
```

#### è‡ªé€‚åº”æ‰¹é‡å¤§å°
```python
def get_optimal_batch_size(self, device: torch.device) -> int:
    """æ ¹æ®è®¾å¤‡ç±»å‹è¿”å›æœ€ä¼˜æ‰¹é‡å¤§å°"""
    if device.type == "cuda":
        return 1024  # GPUå¯ä»¥å¤„ç†å¤§æ‰¹é‡
    elif device.type == "mps":
        return 512   # MPSä¸­ç­‰æ‰¹é‡
    else:
        return 128   # CPUå°æ‰¹é‡
```

### ğŸš« ä¸¥æ ¼ç¦æ­¢

#### ç¡¬ç¼–ç è®¾å¤‡ç±»å‹
```python
# âŒ ç¦æ­¢ï¼šç¡¬ç¼–ç CUDA
x = torch.randn(10, 10).cuda()

# âŒ ç¦æ­¢ï¼šå‡è®¾è®¾å¤‡ç±»å‹
if torch.cuda.is_available():
    # åªè€ƒè™‘CUDAï¼Œå¿½ç•¥MPS
```

#### è®¾å¤‡ç‰¹å®šçš„åŒæ­¥è°ƒç”¨
```python
# âŒ ç¦æ­¢ï¼šåªå¤„ç†CUDAåŒæ­¥
torch.cuda.synchronize()

# âœ… æ­£ç¡®ï¼šé€šç”¨åŒæ­¥å‡½æ•°
def synchronize_device(device: torch.device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()
    # CPUæ— éœ€åŒæ­¥
```

## æµ‹è¯•ç­–ç•¥

### å¤šè®¾å¤‡æµ‹è¯•çŸ©é˜µ
æ‰€æœ‰ç®—æ³•å¿…é¡»åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­é€šè¿‡æµ‹è¯•ï¼š

| è®¾å¤‡ç±»å‹ | å¼€å‘ç¯å¢ƒ | éƒ¨ç½²ç¯å¢ƒ | æµ‹è¯•é‡ç‚¹ |
|---------|---------|---------|---------|
| **MPS** | âœ… M4 Mac | âš ï¸ å°‘è§ | æœ¬åœ°å¼€å‘éªŒè¯ |
| **CUDA** | âŒ ä¸å¯ç”¨ | âœ… Linux | éƒ¨ç½²ç¯å¢ƒéªŒè¯ |
| **CPU** | âœ… åå¤‡ | âœ… åå¤‡ | å…¼å®¹æ€§ä¿è¯ |

### æµ‹è¯•æ ‡è®°
```python
@pytest.mark.mps
def test_flow_matching_mps():
    """MPSè®¾å¤‡ä¸“ç”¨æµ‹è¯•"""
    
@pytest.mark.cuda  
def test_flow_matching_cuda():
    """CUDAè®¾å¤‡ä¸“ç”¨æµ‹è¯•"""
    
@pytest.mark.skipif(not torch.backends.mps.is_available(), reason="éœ€è¦MPSæ”¯æŒ")
def test_apple_silicon_optimization():
    """Apple Siliconç‰¹å®šä¼˜åŒ–æµ‹è¯•"""
```

## æ€§èƒ½åŸºå‡†è¦æ±‚

### è·¨è®¾å¤‡æ€§èƒ½å¯¹æ¯”
æ¯ä¸ªç®—æ³•éƒ½å¿…é¡»æä¾›è·¨è®¾å¤‡æ€§èƒ½åŸºå‡†ï¼š

```python
def benchmark_across_devices():
    """åœ¨å¯ç”¨è®¾å¤‡ä¸Šè¿è¡Œæ€§èƒ½æµ‹è¯•"""
    devices = []
    if torch.cuda.is_available():
        devices.append("cuda")
    if torch.backends.mps.is_available():
        devices.append("mps")
    devices.append("cpu")
    
    for device in devices:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_flow_matching(device)
```

## éƒ¨ç½²ä¼˜åŒ–å»ºè®®

### Linuxéƒ¨ç½²ä¼˜åŒ–
- **CUDAç‰ˆæœ¬**: å»ºè®®CUDA 11.8+ï¼Œæ”¯æŒæœ€æ–°PyTorchç‰¹æ€§
- **é©±åŠ¨ç‰ˆæœ¬**: NVIDIAé©±åŠ¨ç‰ˆæœ¬â‰¥525.60.13
- **å¤šGPU**: æ”¯æŒ`torch.nn.DataParallel`å’Œ`DistributedDataParallel`

### Macå¼€å‘ä¼˜åŒ–  
- **PyTorchç‰ˆæœ¬**: ç¡®ä¿ä½¿ç”¨æ”¯æŒMPSçš„PyTorch 2.0+
- **å†…å­˜ç®¡ç†**: MPSå†…å­˜ä¸æ”¯æŒæŸäº›CUDAå†…å­˜æ“ä½œ
- **è°ƒè¯•å·¥å…·**: ä½¿ç”¨`torch.mps.profiler`è¿›è¡Œæ€§èƒ½åˆ†æ

## é”™è¯¯å¤„ç†

### è®¾å¤‡ä¸å…¼å®¹çš„ä¼˜é›…é™çº§
```python
try:
    # å°è¯•GPUè®¡ç®—
    result = compute_on_gpu(data)
except RuntimeError as e:
    if "out of memory" in str(e).lower():
        # é™çº§åˆ°CPUæˆ–å‡å°‘æ‰¹é‡å¤§å°
        result = compute_with_smaller_batch(data)
    else:
        raise
```

### è®¾å¤‡ç‰¹å®šçš„é”™è¯¯æç¤º
```python
def check_device_requirements(device: torch.device):
    """æ£€æŸ¥è®¾å¤‡ç‰¹å®šè¦æ±‚"""
    if device.type == "mps" and not torch.backends.mps.is_available():
        raise RuntimeError(
            "MPSåç«¯ä¸å¯ç”¨ã€‚è¯·ç¡®ä¿:\n"
            "1. ä½¿ç”¨æ”¯æŒMPSçš„PyTorchç‰ˆæœ¬ (â‰¥2.0)\n" 
            "2. è¿è¡Œåœ¨Apple Silicon Macä¸Š\n"
            "3. macOSç‰ˆæœ¬â‰¥12.3"
        )
```

è¿™äº›è§„åˆ™ç¡®ä¿AllFlowåº“èƒ½å¤Ÿåœ¨M4 Macå¼€å‘ç¯å¢ƒå’ŒLinuxç”Ÿäº§ç¯å¢ƒä¹‹é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨å„å¹³å°çš„è®¡ç®—ä¼˜åŠ¿ã€‚
