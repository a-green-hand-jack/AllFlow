"""AllFlow - é«˜æ•ˆçš„Flow Matchingç®—æ³•åº“

AllFlowæ˜¯ä¸€ä¸ªä¸“æ³¨äºFlow Matchingæ ¸å¿ƒç®—æ³•çš„PyTorchåº“ï¼Œæä¾›ï¼š

- çº¯ç®—æ³•å®ç°ï¼Œä¸ç¥ç»ç½‘ç»œæ¶æ„è§£è€¦
- æè‡´æ€§èƒ½ä¼˜åŒ–ï¼Œé¿å…Pythonå¾ªç¯
- è·¨è®¾å¤‡å…¼å®¹æ€§ï¼ˆCPU/CUDA/MPSï¼‰
- ç§‘å­¦ä¸¥è°¨çš„æ•°å­¦å®ç°
- çµæ´»çš„æ—¶é—´é‡‡æ ·ç­–ç•¥
- ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ç³»ç»Ÿ

ä¸»è¦ç»„ä»¶:
- FlowMatching: æ ‡å‡†Flow Matchingç®—æ³•
- TimeSampler: çµæ´»çš„æ—¶é—´é‡‡æ ·å™¨ï¼ˆå‡åŒ€ã€æ­£æ€ã€æŒ‡æ•°ã€é‡è¦æ€§ï¼‰
- ModelInterface: ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼ˆæ”¯æŒæ¡ä»¶æ¨¡å‹å’Œé¢å¤–å‚æ•°ï¼‰
- ODESolver: é«˜ç²¾åº¦ODEæ±‚è§£å™¨
- å·¥å…·å‡½æ•°: éªŒè¯ã€è®¾å¤‡ç®¡ç†ç­‰

å¿«é€Ÿå¼€å§‹:
    >>> import allflow
    >>> # åŸºç¡€ç”¨æ³•
    >>> flow = allflow.FlowMatching()
    >>> 
    >>> # ä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´é‡‡æ ·å™¨
    >>> sampler = allflow.NormalTimeSampler(mean=0.3, std=0.1)
    >>> flow = allflow.FlowMatching(time_sampler=sampler)
    >>> 
    >>> # ä½¿ç”¨æ¡ä»¶æ¨¡å‹
    >>> model_wrapper = allflow.ConditionalModelWrapper(model, condition=labels)
    >>> loss = flow.compute_loss(x_0, x_1, model=model_wrapper)

Author: AllFlow Contributors
License: MIT
Version: 0.1.0
"""

import logging
from typing import Optional

# æ ¸å¿ƒç®—æ³•å¯¼å…¥
from .algorithms.flow_matching import FlowMatching
from .core.base import FlowMatchingBase, VectorField, PathInterpolation

# æ—¶é—´é‡‡æ ·å™¨å¯¼å…¥
from .core.time_sampling import (
    TimeSamplerBase,
    UniformTimeSampler,
    NormalTimeSampler,
    ExponentialTimeSampler,
    ImportanceTimeSampler,
    create_time_sampler,
)

# æ¨¡å‹æ¥å£å¯¼å…¥
from .core.model_interface import (
    ModelInterface,
    SimpleModelWrapper,
    ConditionalModelWrapper,
    FlexibleModelWrapper,
    FunctionModelWrapper,
    create_model_wrapper,
)

# ODEæ±‚è§£å™¨å¯¼å…¥
from .solvers.base import ODESolverBase, SolverConfig, VectorFieldWrapper

# å°è¯•å¯¼å…¥torchdiffeqæ±‚è§£å™¨ï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    from .solvers.torchdiffeq_solver import TorchDiffEqSolver, EulerSolver
    HAS_TORCHDIFFEQ = True
except ImportError:
    TorchDiffEqSolver = None
    EulerSolver = None
    HAS_TORCHDIFFEQ = False

# å¯¼å…¥æ’å€¼å™¨ç›¸å…³ç±»
from .core.interpolation import (
    EuclideanInterpolation,
    SO3Interpolation,
    create_interpolation,
)

# å¯¼å…¥é€Ÿåº¦åœºç›¸å…³ç±»
from .core.vector_field import (
    EuclideanVectorField,
    SO3VectorField,
    create_vector_field,
)

# å¯¼å…¥å™ªå£°ç”Ÿæˆå™¨ç›¸å…³ç±»
from .core.noise_generators import (
    NoiseGeneratorBase,
    GaussianNoiseGenerator,
    SO3NoiseGenerator,
    UniformNoiseGenerator,
    create_noise_generator,
)

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "0.1.0"
__author__ = "AllFlow Contributors"
__email__ = "allflow@example.com"
__license__ = "MIT"

# å…¬å…±APIå¯¼å‡º
__all__ = [
    # ç‰ˆæœ¬ä¿¡æ¯
    "__version__", "__author__", "__license__",
    
    # æ ¸å¿ƒç®—æ³•
    "FlowMatching",
    "FlowMatchingBase",
    "VectorField", 
    "PathInterpolation",
    
    # æ—¶é—´é‡‡æ ·å™¨
    "TimeSamplerBase",
    "UniformTimeSampler",
    "NormalTimeSampler",
    "ExponentialTimeSampler",
    "ImportanceTimeSampler",
    "create_time_sampler",
    
    # æ¨¡å‹æ¥å£
    "ModelInterface",
    "SimpleModelWrapper",
    "ConditionalModelWrapper",
    "FlexibleModelWrapper",
    "FunctionModelWrapper",
    "create_model_wrapper",
    
    # è·¯å¾„æ’å€¼ (æ–°å¢)
    "EuclideanInterpolation",
    "SO3Interpolation",
    "create_interpolation",
    # é€Ÿåº¦åœºè®¡ç®— (æ–°å¢)
    "EuclideanVectorField",
    "SO3VectorField",
    "create_vector_field",
    # å™ªå£°ç”Ÿæˆå™¨ (æ–°å¢)
    "NoiseGeneratorBase",
    "GaussianNoiseGenerator",
    "SO3NoiseGenerator",
    "UniformNoiseGenerator",
    "create_noise_generator",
    # ODEæ±‚è§£å™¨
    "ODESolverBase",
    "SolverConfig",
    "VectorFieldWrapper",
    
    # å·¥å…·å‡½æ•°
    "get_device_info",
    "set_global_seed",
    "validate_tensor_inputs",
    
    # ä¾¿æ·å‡½æ•°
    "create_flow_matching",
    "create_solver",
]

# æ¡ä»¶å¯¼å‡ºï¼ˆå¦‚æœä¾èµ–å¯ç”¨ï¼‰
if HAS_TORCHDIFFEQ:
    __all__.extend([
        "TorchDiffEqSolver",
        "EulerSolver",
    ])

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def get_device_info() -> dict:
    """è·å–å½“å‰è®¾å¤‡ä¿¡æ¯.
    
    Returns:
        åŒ…å«è®¾å¤‡ä¿¡æ¯çš„å­—å…¸
    """
    import torch
    
    info = {
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "mps_available": hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),
        "cpu_count": torch.get_num_threads(),
    }
    
    if torch.cuda.is_available():
        try:
            cuda_version = torch.version.cuda  # type: ignore
        except AttributeError:
            cuda_version = 'unknown'
        info.update({
            "cuda_version": cuda_version,
            "gpu_count": torch.cuda.device_count(),
            "current_device": torch.cuda.current_device(),
        })
    
    return info


def set_global_seed(seed: int) -> None:
    """è®¾ç½®å…¨å±€éšæœºç§å­.
    
    Args:
        seed: éšæœºç§å­å€¼
    """
    import torch
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿ç¡®å®šæ€§ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    logger.info(f"å…¨å±€éšæœºç§å­è®¾ç½®ä¸º: {seed}")


def validate_tensor_inputs(*tensors, allow_empty: bool = False) -> bool:
    """éªŒè¯å¼ é‡è¾“å…¥çš„æœ‰æ•ˆæ€§.
    
    Args:
        *tensors: è¦éªŒè¯çš„å¼ é‡
        allow_empty: æ˜¯å¦å…è®¸ç©ºå¼ é‡
        
    Returns:
        éªŒè¯æ˜¯å¦é€šè¿‡
        
    Raises:
        ValueError: å½“å¼ é‡æ— æ•ˆæ—¶
    """
    import torch
    
    if not allow_empty and len(tensors) == 0:
        raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªå¼ é‡è¿›è¡ŒéªŒè¯")
    
    for i, tensor in enumerate(tensors):
        if not isinstance(tensor, torch.Tensor):
            raise ValueError(f"ç¬¬{i}ä¸ªè¾“å…¥ä¸æ˜¯torch.Tensorç±»å‹: {type(tensor)}")
        
        if torch.isnan(tensor).any():
            raise ValueError(f"ç¬¬{i}ä¸ªå¼ é‡åŒ…å«NaNå€¼")
        
        if torch.isinf(tensor).any():
            raise ValueError(f"ç¬¬{i}ä¸ªå¼ é‡åŒ…å«Infå€¼")
    
    return True


def create_flow_matching(
    device: Optional[str] = None,
    dtype: Optional[str] = None,
    **kwargs
) -> FlowMatching:
    """åˆ›å»ºFlow Matchingå®ä¾‹çš„ä¾¿æ·å‡½æ•°.
    
    Args:
        device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚'cuda'ã€'mps'æˆ–'cpu'
        dtype: æ•°æ®ç±»å‹ï¼Œå¦‚'float32'
        **kwargs: ä¼ é€’ç»™FlowMatchingçš„å…¶ä»–å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„FlowMatchingå®ä¾‹
        
    Example:
        >>> flow = allflow.create_flow_matching(device='cuda')
        >>> # å¼€å§‹ä½¿ç”¨flowè¿›è¡Œè®­ç»ƒ
    """
    import torch
    
    # è½¬æ¢dtypeå­—ç¬¦ä¸²ä¸ºtorchç±»å‹
    torch_dtype = None
    if dtype is not None:
        if isinstance(dtype, str):
            torch_dtype = getattr(torch, dtype)
        else:
            torch_dtype = dtype
    
    return FlowMatching(device=device, dtype=torch_dtype, **kwargs)


def create_solver(
    solver_type: str = "torchdiffeq",
    method: str = "dopri5",
    **kwargs
) -> ODESolverBase:
    """åˆ›å»ºODEæ±‚è§£å™¨çš„ä¾¿æ·å‡½æ•°.
    
    Args:
        solver_type: æ±‚è§£å™¨ç±»å‹ï¼Œ'torchdiffeq'æˆ–'euler'
        method: æ•°å€¼æ–¹æ³•åç§°
        **kwargs: ä¼ é€’ç»™æ±‚è§£å™¨çš„å…¶ä»–å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„ODEæ±‚è§£å™¨å®ä¾‹
        
    Raises:
        ImportError: å½“è¯·æ±‚çš„æ±‚è§£å™¨ä¸å¯ç”¨æ—¶
        ValueError: å½“solver_typeä¸æ”¯æŒæ—¶
    """
    if solver_type == "torchdiffeq":
        if not HAS_TORCHDIFFEQ:
            raise ImportError(
                "torchdiffeqä¸å¯ç”¨ã€‚è¯·å®‰è£…: pip install torchdiffeq"
            )
        return TorchDiffEqSolver(method=method, **kwargs)  # type: ignore
    
    elif solver_type == "euler":
        if not HAS_TORCHDIFFEQ:
            raise ImportError(
                "Euleræ±‚è§£å™¨éœ€è¦torchdiffeqã€‚è¯·å®‰è£…: pip install torchdiffeq"
            )
        return EulerSolver(**kwargs)  # type: ignore
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ±‚è§£å™¨ç±»å‹: {solver_type}")


def _check_dependencies() -> None:
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–å¹¶æ¨èæœ€ä¼˜è®¡ç®—åç«¯."""
    try:
        import torch
        
        # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§å¹¶ç»™å‡ºå»ºè®®
        available_devices = []
        performance_info = []
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            available_devices.append(f"CUDA (GPU: {gpu_count}ä¸ª)")
            performance_info.append("âœ… CUDA GPUå¯ç”¨ï¼Œå°†è·å¾—æœ€ä½³æ€§èƒ½")
        
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            available_devices.append("MPS (Apple Silicon)")
            performance_info.append("âœ… Apple Silicon MPSå¯ç”¨ï¼Œé’ˆå¯¹Mç³»åˆ—èŠ¯ç‰‡ä¼˜åŒ–")
        
        available_devices.append("CPU")
        
        if len(available_devices) == 1:  # åªæœ‰CPU
            import warnings
            warnings.warn(
                "âš ï¸  åªæœ‰CPUåç«¯å¯ç”¨ï¼Œæ€§èƒ½å¯èƒ½å—é™ã€‚\n"
                "å»ºè®®ï¼š\n"
                "â€¢ Linux: å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch\n"
                "â€¢ Mac: ç¡®ä¿ä½¿ç”¨æ”¯æŒMPSçš„PyTorch 2.0+",
                UserWarning
            )
        else:
            # æœ‰GPUæˆ–MPSåŠ é€Ÿ
            import logging
            logging.info(f"ğŸš€ AllFlowå·²æ£€æµ‹åˆ°åŠ é€Ÿè®¡ç®—åç«¯: {', '.join(available_devices)}")
            for info in performance_info:
                logging.info(info)
                
    except ImportError:
        raise ImportError(
            "AllFlowéœ€è¦PyTorch>=2.0.0ã€‚è¯·æ ¹æ®æ‚¨çš„å¹³å°å®‰è£…ï¼š\n\n"
            "ğŸ“± Apple Silicon Mac:\n"
            "   pip install torch>=2.0.0\n\n"
            "ğŸ§ Linux with CUDA:\n"
            "   pip install torch>=2.0.0 --index-url https://download.pytorch.org/whl/cu118\n\n"
            "ğŸ’» CPU only:\n"
            "   pip install torch>=2.0.0 --index-url https://download.pytorch.org/whl/cpu"
        )


# åœ¨å¯¼å…¥æ—¶æ£€æŸ¥ä¾èµ–
_check_dependencies() 